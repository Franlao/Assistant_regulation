"""
Exemple de Migration vers LangGraph
===================================

Exemple complet montrant comment migrer du ModularOrchestrator
vers LangGraphOrchestrator avec compatibilit√© totale.
"""

import os
import sys
import time

# Ajouter le chemin du projet
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import des orchestrateurs
from assistant_regulation.planning.langgraph.orchestrator import LangGraphOrchestrator
from assistant_regulation.planning.langgraph.compatibility_adapter import LangGraphCompatibilityAdapter


def example_migration_progressive():
    """
    Exemple de migration progressive utilisant l'adaptateur de compatibilit√©.
    """
    print("=== Exemple de Migration Progressive ===\\n")
    
    # 1. Utilisation de l'adaptateur de compatibilit√©
    print("1. Initialisation avec adaptateur de compatibilit√©...")
    
    # L'adaptateur permet d'utiliser LangGraph avec fallback vers ModularOrchestrator
    orchestrator = LangGraphCompatibilityAdapter(
        llm_provider="ollama",
        model_name="llama3.2",
        enable_verification=True,
        workflow_type="full",  # LangGraph workflow
        fallback_to_modular=True  # Fallback si LangGraph √©choue
    )
    
    # V√©rifier le statut des orchestrateurs
    status = orchestrator.get_orchestrator_status()
    print(f"Statut: {status}\\n")
    
    # 2. Test de requ√™te - Interface identique au ModularOrchestrator
    print("2. Test de requ√™te avec interface compatible...")
    
    query = "Quels sont les exigences pour les syst√®mes de freinage d'urgence ?"
    
    try:
        result = orchestrator.process_query(
            query=query,
            use_images=True,
            use_tables=True,
            top_k=5,
            use_conversation_context=True,
            use_advanced_routing=True
        )
        
        print(f"R√©ponse g√©n√©r√©e par: {result['metadata'].get('orchestrator', 'unknown')}")
        print(f"Succ√®s: {result['success']}")
        print(f"Longueur r√©ponse: {len(result['answer'])} caract√®res")
        print(f"Nombre de sources: {len(result['sources'])}")
        print(f"Temps de traitement: {result['metadata'].get('processing_time', 0):.2f}s\\n")
        
    except Exception as e:
        print(f"Erreur: {e}\\n")


def example_direct_langgraph():
    """
    Exemple d'utilisation directe de LangGraphOrchestrator.
    """
    print("=== Exemple d'Utilisation Directe de LangGraph ===\\n")
    
    # 1. Initialisation directe de LangGraph
    print("1. Initialisation de LangGraphOrchestrator...")
    
    try:
        orchestrator = LangGraphOrchestrator(
            llm_provider="ollama",
            model_name="llama3.2",
            enable_verification=True,
            workflow_type="full",  # Workflow complet
            enable_debug=False
        )
        
        print("LangGraphOrchestrator initialis√© avec succ√®s\\n")
        
        # 2. Informations sur l'architecture
        print("2. Informations sur l'architecture...")
        stats = orchestrator.get_agent_statistics()
        print(f"Type de workflow: {stats['workflow_type']}")
        print(f"Agents initialis√©s: {stats['agents_initialized']}")
        print(f"Services actifs: {stats['services_status']}\\n")
        
        # 3. Test de routage
        print("3. Test d'analyse de routage...")
        query = "Comment tester la r√©sistance des mat√©riaux selon R046 ?"
        
        routing_info = orchestrator.get_routing_info(query)
        print(f"Plan d'ex√©cution: {routing_info}")
        
        explanation = orchestrator.explain_routing_decision(query)
        print(f"Explication: {explanation}\\n")
        
        # 4. Traitement de requ√™te compl√®te
        print("4. Traitement de requ√™te compl√®te...")
        
        result = orchestrator.process_query(
            query=query,
            use_images=True,
            use_tables=True,
            top_k=3
        )
        
        print(f"R√©ponse: {result['answer'][:200]}...")
        print(f"Sources trouv√©es: {len(result['sources'])}")
        print(f"Images trouv√©es: {len(result['images'])}")
        print(f"Tableaux trouv√©s: {len(result['tables'])}")
        
        # M√©tadonn√©es de traitement
        metadata = result['metadata']
        print(f"\\nM√©tadonn√©es de traitement:")
        print(f"- Temps total: {metadata.get('processing_time', 0):.2f}s")
        print(f"- Trace d'agents: {metadata.get('agent_trace', [])}")
        print(f"- Analyse de requ√™te: {metadata.get('query_analysis', {})}")
        
    except ImportError as e:
        print(f"LangGraph non disponible: {e}")
        print("Assurez-vous d'installer: pip install langgraph langchain")
    except Exception as e:
        print(f"Erreur lors de l'initialisation: {e}")


def example_streaming():
    """
    Exemple de traitement en streaming avec LangGraph.
    """
    print("\\n=== Exemple de Streaming avec LangGraph ===\\n")
    
    try:
        # Utiliser l'adaptateur avec pr√©f√©rence pour LangGraph
        orchestrator = LangGraphCompatibilityAdapter(
            llm_provider="ollama",
            model_name="llama3.2",
            workflow_type="streaming",  # Workflow optimis√© pour streaming
            fallback_to_modular=True
        )
        
        query = "Expliquez les proc√©dures de test de s√©curit√© pour les v√©hicules √©lectriques"
        
        print(f"Streaming pour: {query}\\n")
        
        for event in orchestrator.process_query_stream(query=query, top_k=3):
            print(f"[{event.get('type', 'unknown')}] {event.get('message', event.get('content', str(event)))}")
            
            # Simuler un d√©lai pour voir le streaming
            time.sleep(0.1)
            
    except Exception as e:
        print(f"Erreur streaming: {e}")


def example_comparison():
    """
    Exemple de comparaison entre ModularOrchestrator et LangGraphOrchestrator.
    """
    print("\\n=== Comparaison des Orchestrateurs ===\\n")
    
    # Utiliser l'adaptateur pour faciliter la comparaison
    adapter = LangGraphCompatibilityAdapter(
        llm_provider="ollama",
        model_name="llama3.2",
        fallback_to_modular=True
    )
    
    # Test de comparaison sur une requ√™te
    query = "Quelles sont les normes de s√©curit√© pour les syst√®mes d'√©clairage ?"
    
    print(f"Test comparatif pour: {query}\\n")
    
    try:
        comparison_results = adapter.run_comparison_test(query)
        
        print("R√©sultats comparatifs:")
        for orchestrator_name, results in comparison_results["results"].items():
            print(f"\\n{orchestrator_name.upper()}:")
            if results.get("success"):
                print(f"  ‚úì Succ√®s")
                print(f"  ‚è±Ô∏è  Temps: {results['processing_time']:.2f}s")
                print(f"  üìù Longueur r√©ponse: {results['answer_length']} caract√®res")
                print(f"  üìö Sources: {results['sources_count']}")
            else:
                print(f"  ‚ùå √âchec: {results.get('error')}")
        
    except Exception as e:
        print(f"Erreur lors de la comparaison: {e}")


def example_workflow_switching():
    """
    Exemple de changement dynamique de workflow.
    """
    print("\\n=== Changement Dynamique de Workflow ===\\n")
    
    try:
        orchestrator = LangGraphOrchestrator(
            llm_provider="ollama",
            model_name="llama3.2",
            workflow_type="full"
        )
        
        query = "Test de workflow switching"
        
        # Test avec workflow full
        print("1. Workflow 'full':")
        stats1 = orchestrator.get_agent_statistics()
        print(f"   Type actuel: {stats1['workflow_type']}")
        
        # Changer vers workflow simple
        print("\\n2. Changement vers workflow 'simple':")
        orchestrator.switch_workflow("simple")
        stats2 = orchestrator.get_agent_statistics()
        print(f"   Nouveau type: {stats2['workflow_type']}")
        
        # Changer vers workflow debug
        print("\\n3. Changement vers workflow 'debug':")
        orchestrator.switch_workflow("debug")
        stats3 = orchestrator.get_agent_statistics()
        print(f"   Type debug: {stats3['workflow_type']}")
        
        print("\\nChangements de workflow r√©ussis !")
        
    except Exception as e:
        print(f"Erreur lors du changement de workflow: {e}")


def main():
    """
    Fonction principale ex√©cutant tous les exemples.
    """
    print("üöÄ Exemples de Migration vers LangGraph\\n")
    print("=" * 60)
    
    # 1. Migration progressive
    example_migration_progressive()
    
    # 2. Utilisation directe
    example_direct_langgraph()
    
    # 3. Streaming
    example_streaming()
    
    # 4. Comparaison
    example_comparison()
    
    # 5. Changement de workflow
    example_workflow_switching()
    
    print("\\n" + "=" * 60)
    print("‚úÖ Tous les exemples termin√©s!")


if __name__ == "__main__":
    main()